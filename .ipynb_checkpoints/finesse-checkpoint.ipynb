{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# from estnltk.default_resolver import DEFAULT_RESOLVER\n",
    "\n",
    "# from estnltk.default_resolver import make_resolver\n",
    "# from estnltk.vabamorf.morf import syllabify_words\n",
    "from praatio import textgrid\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "grid = textgrid.openTextgrid(\"/Users/sarah/qp_final/txtgridtest/55.TextGrid\",True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #import textgrid\n",
    "# grid_df = pd.DataFrame(grid_dict)\n",
    "# grid_df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_io.TextIOWrapper' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/sarah/qp_final/finesse.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sarah/qp_final/finesse.ipynb#ch0000001?line=25'>26</a>\u001b[0m Q2 \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sarah/qp_final/finesse.ipynb#ch0000001?line=26'>27</a>\u001b[0m Q1 \u001b[39m=\u001b[39m []\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sarah/qp_final/finesse.ipynb#ch0000001?line=27'>28</a>\u001b[0m \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m words: \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sarah/qp_final/finesse.ipynb#ch0000001?line=28'>29</a>\u001b[0m     syll \u001b[39m=\u001b[39m syllabify_word(word,as_dict\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sarah/qp_final/finesse.ipynb#ch0000001?line=29'>30</a>\u001b[0m     tmp \u001b[39m=\u001b[39m {word:syll}\n",
      "File \u001b[0;32m~/miniconda3/envs/eesti_nltk/lib/python3.9/site-packages/nltk/tokenize/punkt.py:560\u001b[0m, in \u001b[0;36mPunktBaseClass._tokenize_words\u001b[0;34m(self, plaintext)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/sarah/miniconda3/envs/eesti_nltk/lib/python3.9/site-packages/nltk/tokenize/punkt.py?line=551'>552</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/sarah/miniconda3/envs/eesti_nltk/lib/python3.9/site-packages/nltk/tokenize/punkt.py?line=552'>553</a>\u001b[0m \u001b[39mDivide the given text into tokens, using the punkt word\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/sarah/miniconda3/envs/eesti_nltk/lib/python3.9/site-packages/nltk/tokenize/punkt.py?line=553'>554</a>\u001b[0m \u001b[39msegmentation regular expression, and generate the resulting list\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/sarah/miniconda3/envs/eesti_nltk/lib/python3.9/site-packages/nltk/tokenize/punkt.py?line=556'>557</a>\u001b[0m \u001b[39mrespectively.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/sarah/miniconda3/envs/eesti_nltk/lib/python3.9/site-packages/nltk/tokenize/punkt.py?line=557'>558</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/sarah/miniconda3/envs/eesti_nltk/lib/python3.9/site-packages/nltk/tokenize/punkt.py?line=558'>559</a>\u001b[0m parastart \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/sarah/miniconda3/envs/eesti_nltk/lib/python3.9/site-packages/nltk/tokenize/punkt.py?line=559'>560</a>\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m plaintext\u001b[39m.\u001b[39;49msplit(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    <a href='file:///Users/sarah/miniconda3/envs/eesti_nltk/lib/python3.9/site-packages/nltk/tokenize/punkt.py?line=560'>561</a>\u001b[0m     \u001b[39mif\u001b[39;00m line\u001b[39m.\u001b[39mstrip():\n\u001b[1;32m    <a href='file:///Users/sarah/miniconda3/envs/eesti_nltk/lib/python3.9/site-packages/nltk/tokenize/punkt.py?line=561'>562</a>\u001b[0m         line_toks \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lang_vars\u001b[39m.\u001b[39mword_tokenize(line))\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_io.TextIOWrapper' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "import estnltk\n",
    "from estnltk.vabamorf.morf import syllabify_word\n",
    "\n",
    "import nltk.data\n",
    "tok = nltk.data.load('tokenizers/punkt/estonian.pickle')\n",
    "\n",
    "resolver = estnltk.default_resolver.make_resolver(\n",
    "                 disambiguate=False,\n",
    "                 guess=False,\n",
    "                 propername=False,\n",
    "                 phonetic=True,\n",
    "                 compound=True,\n",
    "                 slang_lex=True)\n",
    "song = open(\"/Users/sarah/qp_final/009.txt\",'r')\n",
    "\n",
    "\n",
    "\n",
    "# song_x = estnltk.Text(song.readlines())\n",
    "# words = tok.tokenize(song_x.text)\n",
    "\n",
    "\n",
    "song_y = estnltk.Text(\"Laula, laula, suukkene,liigu, linnu keelekkene,m천lgu, marja meelekkene,iluttse, s체d채mekkene! K체ll sa siis saad vaitt olla, kui saad alla mustta mulla, valge lauade vahele, kena kirsttu keskke'elle!\")\n",
    "words = tok._tokenize_words(song)\n",
    "syllDict = {}\n",
    "Q3 = []\n",
    "Q2 = []\n",
    "Q1 = []\n",
    "for item in words: \n",
    "    syll = syllabify_word(word,as_dict=False)\n",
    "    tmp = {word:syll}\n",
    "\n",
    "    if syll[1] == 3 : Q3.append(tmp)\n",
    "    if syll[1] == 2 : Q2.append(tmp)\n",
    "    else:  Q1.append(tmp)\n",
    "    syllDict.update({word:syll})\n",
    "\n",
    "\n",
    "# words\n",
    "\n",
    "# print(song_y.words)\n",
    "# song_y.tag_layer('words')\n",
    "# song_y.tag_layer('morph_analysis')\n",
    "# song_y.tag_layer(resolver=resolver)['morph_analysis']\n",
    "# print(song_y.words)\n",
    "# syll = syllabify_words(song_x.words.text,as_dict=True)\n",
    "\n",
    "# tok.tokenize(song_x.text)\n",
    "\n",
    "\n",
    "\n",
    "syllDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/sarah/qp_final/finesse.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sarah/qp_final/finesse.ipynb#ch0000002?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenize\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mregexp\u001b[39;00m \u001b[39mimport\u001b[39;00m WordPunctTokenizer\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sarah/qp_final/finesse.ipynb#ch0000002?line=1'>2</a>\u001b[0m tok \u001b[39m=\u001b[39m WordPunctTokenizer()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sarah/qp_final/finesse.ipynb#ch0000002?line=3'>4</a>\u001b[0m tok\u001b[39m.\u001b[39;49mtokenize(song_x[\u001b[39m'\u001b[39;49m\u001b[39mwords\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[0;32m~/miniconda3/envs/eesti_nltk/lib/python3.9/site-packages/nltk/tokenize/regexp.py:133\u001b[0m, in \u001b[0;36mRegexpTokenizer.tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/sarah/miniconda3/envs/eesti_nltk/lib/python3.9/site-packages/nltk/tokenize/regexp.py?line=128'>129</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_regexp\u001b[39m.\u001b[39msplit(text)\n\u001b[1;32m    <a href='file:///Users/sarah/miniconda3/envs/eesti_nltk/lib/python3.9/site-packages/nltk/tokenize/regexp.py?line=130'>131</a>\u001b[0m \u001b[39m# If our regexp matches tokens, use re.findall:\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/sarah/miniconda3/envs/eesti_nltk/lib/python3.9/site-packages/nltk/tokenize/regexp.py?line=131'>132</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/sarah/miniconda3/envs/eesti_nltk/lib/python3.9/site-packages/nltk/tokenize/regexp.py?line=132'>133</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_regexp\u001b[39m.\u001b[39;49mfindall(text)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize.regexp import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "\n",
    "tok.tokenize(song_x['])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dd9a263a021b7496b06352cb4600e46858ffaf775c6cfeaeaf5f4e380d7237f2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
